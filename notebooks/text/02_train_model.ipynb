{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52b8c5f",
   "metadata": {},
   "source": [
    "# 02_train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1486ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: C:\\Users\\viach\\Downloads\\document-classifier-portfolio-v2\\project_config.json\n",
      "Train: (42000, 4) Val: (9000, 4)\n",
      "doc_type\n",
      "SCIENTIFIC_PAPER    14000\n",
      "EMAIL               14000\n",
      "INVOICE             14000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "HERE = Path.cwd().resolve()\n",
    "\n",
    "def find_config(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        cand = p / \"project_config.json\"\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    raise FileNotFoundError(\"project_config.json not found. Run 00_config_and_checks.ipynb first.\")\n",
    "\n",
    "CONFIG_PATH = find_config(HERE)\n",
    "cfg = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "print(\"Config:\", CONFIG_PATH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PROCESSED_DIR = Path(cfg[\"PROCESSED_DIR\"])\n",
    "MODELS_DIR = Path(cfg[\"MODELS_DIR\"])\n",
    "\n",
    "train_df = pd.read_csv(PROCESSED_DIR / \"train.csv\")\n",
    "val_df = pd.read_csv(PROCESSED_DIR / \"val.csv\")\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Val:\", val_df.shape)\n",
    "print(train_df[\"doc_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1298be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoice augmentation (train only)\n",
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "def aug_invoice(text: str) -> str:\n",
    "    t = text\n",
    "    # reorder lines\n",
    "    lines = [ln for ln in t.splitlines() if ln.strip()]\n",
    "    if len(lines) > 3:\n",
    "        idx = np.arange(len(lines))\n",
    "        RNG.shuffle(idx)\n",
    "        lines = [lines[i] for i in idx]\n",
    "        t = \"\\n\".join(lines)\n",
    "    # mild noise\n",
    "    if RNG.random() < 0.35:\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    if RNG.random() < 0.25:\n",
    "        t = t.replace(\":\", \" : \")\n",
    "    if RNG.random() < 0.25:\n",
    "        t += \"\\nPayment terms: Pay within 30 days.\"\n",
    "    if RNG.random() < 0.15:\n",
    "        t += \"\\nVAT: 7.7%\"\n",
    "    return t.strip()\n",
    "\n",
    "def augment_invoices(df: pd.DataFrame, factor: int = 2) -> pd.DataFrame:\n",
    "    inv = df[df[\"doc_type\"] == \"INVOICE\"].copy()\n",
    "    if inv.empty or factor <= 1:\n",
    "        return df\n",
    "    base = inv[\"text\"].astype(str).tolist()\n",
    "    rows = []\n",
    "    for _ in range((factor - 1) * len(base)):\n",
    "        b = base[RNG.integers(0, len(base))]\n",
    "        rows.append({\"text\": aug_invoice(b), \"doc_type\": \"INVOICE\", \"source\": \"SYN_INVOICE\"})\n",
    "    aug = pd.DataFrame(rows)\n",
    "    out = pd.concat([df, aug], ignore_index=True)\n",
    "    out = out.drop_duplicates(subset=[\"doc_type\", \"text\"]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222fb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2a43a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train use: (42000, 4)\n",
      "doc_type\n",
      "SCIENTIFIC_PAPER    14000\n",
      "EMAIL               14000\n",
      "INVOICE             14000\n",
      "Name: count, dtype: int64\n",
      "Val use: (9000, 4)\n",
      "Fitting...\n",
      "Fit time (s): 10.6\n",
      "macro_f1: 0.9991\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           EMAIL      1.000     0.998     0.999      3000\n",
      "         INVOICE      1.000     1.000     1.000      3000\n",
      "SCIENTIFIC_PAPER      0.998     1.000     0.999      3000\n",
      "\n",
      "        accuracy                          0.999      9000\n",
      "       macro avg      0.999     0.999     0.999      9000\n",
      "    weighted avg      0.999     0.999     0.999      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# 1) Keep training size under control (dev-friendly, reproducible)\n",
    "MAX_PER_CLASS = None  # set to None to use all\n",
    "\n",
    "if MAX_PER_CLASS is not None:\n",
    "    train_use = (\n",
    "        train_aug.groupby(\"doc_type\", group_keys=False)\n",
    "        .apply(lambda g: g.sample(n=min(len(g), MAX_PER_CLASS), random_state=42))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    val_use = (\n",
    "        val_df.groupby(\"doc_type\", group_keys=False)\n",
    "        .apply(lambda g: g.sample(n=min(len(g), MAX_PER_CLASS // 4), random_state=42))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    train_use = train_aug\n",
    "    val_use = val_df\n",
    "\n",
    "print(\"Train use:\", train_use.shape)\n",
    "print(train_use[\"doc_type\"].value_counts())\n",
    "print(\"Val use:\", val_use.shape)\n",
    "\n",
    "# 2) Data arrays\n",
    "X_train = train_use[\"text\"].astype(str).to_numpy()\n",
    "y_train = train_use[\"doc_type\"].astype(str).to_numpy()\n",
    "X_val = val_use[\"text\"].astype(str).to_numpy()\n",
    "y_val = val_use[\"doc_type\"].astype(str).to_numpy()\n",
    "\n",
    "# 3) One strong baseline first (word unigrams). Bigrams + char-ngrams can come later as an experiment.\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        max_features=30000,     # smaller vocab = much faster\n",
    "        ngram_range=(1, 1),     # unigrams first (bigrams are expensive)\n",
    "        min_df=5,               # ignore rare terms early\n",
    "        max_df=0.95,            # drop ultra-common terms\n",
    "        stop_words=\"english\",\n",
    "        sublinear_tf=True,\n",
    "        dtype=np.float32\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        solver=\"saga\",          # good for big sparse matrices\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\",\n",
    "        C=2.0\n",
    "    )),\n",
    "])\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Fitting...\")\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"Fit time (s):\", round(time.time() - t0, 1))\n",
    "\n",
    "pred = pipe.predict(X_val)\n",
    "f1 = f1_score(y_val, pred, average=\"macro\")\n",
    "print(\"macro_f1:\", round(f1, 4))\n",
    "print(classification_report(y_val, pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cdb2b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_word_logreg macro_f1: 0.9985553475696106\n",
      "tfidf_char_logreg macro_f1: 0.9985555507006009\n",
      "Best: tfidf_char_logreg\n"
     ]
    }
   ],
   "source": [
    "X_train = train_aug[\"text\"].astype(str).values\n",
    "y_train = train_aug[\"doc_type\"].astype(str).values\n",
    "X_val = val_df[\"text\"].astype(str).values\n",
    "y_val = val_df[\"doc_type\"].astype(str).values\n",
    "\n",
    "candidates = {\n",
    "    \"tfidf_word_logreg\": Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1,2), min_df=3, stop_words=\"english\")),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, n_jobs=-1, class_weight=\"balanced\", C=1.0)),\n",
    "    ]),\n",
    "    \"tfidf_char_logreg\": Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=3)),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, n_jobs=-1, class_weight=\"balanced\", C=1.0)),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "scores = []\n",
    "for name, pipe in candidates.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_val)\n",
    "    f1 = f1_score(y_val, pred, average=\"macro\")\n",
    "    scores.append((name, f1))\n",
    "    print(name, \"macro_f1:\", f1)\n",
    "\n",
    "best_name = sorted(scores, key=lambda x: -x[1])[0][0]\n",
    "best_model = candidates[best_name]\n",
    "best_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "print(\"Best:\", best_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb2a37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\viach\\Downloads\\document-classifier-portfolio-v2\\models\\text_model\\v1\\model.pkl\n"
     ]
    }
   ],
   "source": [
    "import json, joblib\n",
    "from pathlib import Path\n",
    "\n",
    "VERSION = \"v1\"  \n",
    "\n",
    "OUT_DIR = MODELS_DIR / \"text_model\" / VERSION\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = OUT_DIR / \"model.pkl\"\n",
    "joblib.dump(pipe, model_path)   # 'pipe' is the trained Pipeline\n",
    "\n",
    "meta = {\n",
    "    \"model\": \"tfidf_logreg\",\n",
    "    \"vectorizer\": {\n",
    "        \"type\": \"tfidf\",\n",
    "        \"max_features\": pipe.named_steps[\"tfidf\"].max_features,\n",
    "        \"ngram_range\": pipe.named_steps[\"tfidf\"].ngram_range,\n",
    "        \"min_df\": pipe.named_steps[\"tfidf\"].min_df,\n",
    "        \"max_df\": pipe.named_steps[\"tfidf\"].max_df,\n",
    "        \"stop_words\": pipe.named_steps[\"tfidf\"].stop_words,\n",
    "    },\n",
    "    \"classifier\": {\n",
    "        \"type\": \"logreg\",\n",
    "        \"solver\": pipe.named_steps[\"clf\"].solver,\n",
    "        \"C\": pipe.named_steps[\"clf\"].C,\n",
    "        \"class_weight\": pipe.named_steps[\"clf\"].class_weight,\n",
    "        \"max_iter\": pipe.named_steps[\"clf\"].max_iter,\n",
    "    },\n",
    "    \"classes\": list(getattr(pipe.named_steps[\"clf\"], \"classes_\", [])),\n",
    "    \"version\": VERSION\n",
    "}\n",
    "\n",
    "(Path(OUT_DIR) / \"meta.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3588e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
