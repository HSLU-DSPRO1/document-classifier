{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "019d5c7e",
   "metadata": {},
   "source": [
    "# 01_build_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf1aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: C:\\Users\\viach\\Downloads\\document-classifier-portfolio-v2\\project_config.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, re, hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "HERE = Path.cwd().resolve()\n",
    "\n",
    "def find_config(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        cand = p / \"project_config.json\"\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    raise FileNotFoundError(\"project_config.json not found. Run 00_config_and_checks.ipynb first.\")\n",
    "\n",
    "CONFIG_PATH = find_config(HERE)\n",
    "cfg = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "print(\"Config:\", CONFIG_PATH)\n",
    "\n",
    "\n",
    "\n",
    "INPUT_DIR = Path(cfg[\"INPUT_DIR\"])\n",
    "PROCESSED_DIR = Path(cfg[\"PROCESSED_DIR\"])\n",
    "\n",
    "ENRON_CSV = Path(cfg[\"ENRON_CSV\"])\n",
    "INVOICE1_CSV = Path(cfg[\"INVOICE1_CSV\"])\n",
    "INVOICE2_CSV = Path(cfg[\"INVOICE2_CSV\"])\n",
    "ARXIV_PATH = Path(cfg[\"ARXIV_PATH\"])\n",
    "\n",
    "RNG = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff275cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(t: str) -> str:\n",
    "    if not isinstance(t, str):\n",
    "        return \"\"\n",
    "    t = t.replace(\"\\x00\", \" \")\n",
    "    t = re.sub(r\"[\\r\\t]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def filter_by_length(df: pd.DataFrame, col: str = \"text\", min_chars: int = 80, max_chars: int = 12000) -> pd.DataFrame:\n",
    "    n = df[col].astype(str).str.len()\n",
    "    return df[(n >= min_chars) & (n <= max_chars)].copy()\n",
    "\n",
    "def stable_id(text: str) -> str:\n",
    "    h = hashlib.sha256(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "    return h[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738f664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_body(raw_msg: str) -> str:\n",
    "    if not isinstance(raw_msg, str):\n",
    "        return \"\"\n",
    "    parts = re.split(r\"\\n\\s*\\n\", raw_msg, maxsplit=1)\n",
    "    body = parts[1] if len(parts) > 1 else raw_msg\n",
    "    lines = []\n",
    "    for line in body.splitlines():\n",
    "        s = line.strip()\n",
    "        if s.startswith(\">\"):\n",
    "            continue\n",
    "        if re.match(r\"^\\s*On .+ wrote:\\s*$\", line):\n",
    "            continue\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def load_enron(path: Path, max_rows: int | None = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    text_col = None\n",
    "    for c in [\"message\", \"text\", \"content\", \"body\"]:\n",
    "        if c in df.columns:\n",
    "            text_col = c\n",
    "            break\n",
    "    if text_col is None:\n",
    "        obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "        if not obj_cols:\n",
    "            raise ValueError(\"No text column found in Enron CSV.\")\n",
    "        text_col = obj_cols[0]\n",
    "\n",
    "    if max_rows and len(df) > max_rows:\n",
    "        df = df.sample(max_rows, random_state=42)\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"text\"] = df[text_col].astype(str).map(extract_email_body).map(basic_clean)\n",
    "    out[\"doc_type\"] = \"EMAIL\"\n",
    "    out[\"source\"] = \"ENRON\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1c46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "INVOICE1_REQUIRED = [\n",
    "    \"id_invoice\", \"issuedDate\", \"country\", \"service\", \"total\",\n",
    "    \"discount\", \"tax\", \"invoiceStatus\", \"balance\", \"dueDate\", \"client\"\n",
    "]\n",
    "\n",
    "def _safe(row: dict, key: str, default: str = \"\") -> str:\n",
    "    v = row.get(key, default)\n",
    "    if pd.isna(v):\n",
    "        return default\n",
    "    return str(v).strip()\n",
    "\n",
    "def invoice1_row_to_text(row: dict) -> str:\n",
    "    inv_id = _safe(row, \"id_invoice\")\n",
    "    issued = _safe(row, \"issuedDate\")\n",
    "    country = _safe(row, \"country\")\n",
    "    service = _safe(row, \"service\")\n",
    "    total = _safe(row, \"total\")\n",
    "    discount = _safe(row, \"discount\")\n",
    "    tax = _safe(row, \"tax\")\n",
    "    status = _safe(row, \"invoiceStatus\")\n",
    "    balance = _safe(row, \"balance\")\n",
    "    due = _safe(row, \"dueDate\")\n",
    "    client = _safe(row, \"client\")\n",
    "\n",
    "    lines = [\n",
    "        f\"INVOICE {inv_id}\".strip(),\n",
    "        f\"Issued: {issued}\".strip(),\n",
    "        f\"Client: {client}\".strip(),\n",
    "        f\"Country: {country}\".strip(),\n",
    "        f\"Service: {service}\".strip(),\n",
    "        f\"Total: {total}\".strip(),\n",
    "        f\"Discount: {discount}\".strip(),\n",
    "        f\"Tax: {tax}\".strip(),\n",
    "        f\"Balance: {balance}\".strip(),\n",
    "        f\"Status: {status}\".strip(),\n",
    "        f\"Due: {due}\".strip(),\n",
    "    ]\n",
    "    lines = [ln for ln in lines if ln.split(\":\")[-1].strip() != \"\"]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def load_invoices_1(path: Path, max_rows: int | None = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    missing = [c for c in INVOICE1_REQUIRED if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing invoice1 columns: {missing}\")\n",
    "    if max_rows:\n",
    "        df = df.head(max_rows)\n",
    "    texts = df.apply(lambda r: invoice1_row_to_text(r.to_dict()), axis=1)\n",
    "    out = pd.DataFrame({\"text\": texts.map(basic_clean)})\n",
    "    out[\"doc_type\"] = \"INVOICE\"\n",
    "    out[\"source\"] = \"INVOICE_SET_1\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d84fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_REQUIRED = [\n",
    "    \"first_name\",\"last_name\",\"email\",\"product_id\",\"qty\",\"amount\",\n",
    "    \"invoice_date\",\"address\",\"city\",\"stock_code\",\"job\"\n",
    "]\n",
    "\n",
    "def invoice2_row_to_text(row: dict) -> str:\n",
    "    fn = _safe(row, \"first_name\")\n",
    "    ln = _safe(row, \"last_name\")\n",
    "    email = _safe(row, \"email\")\n",
    "    product_id = _safe(row, \"product_id\")\n",
    "    qty = _safe(row, \"qty\")\n",
    "    amount = _safe(row, \"amount\")\n",
    "    inv_date = _safe(row, \"invoice_date\")\n",
    "    addr = _safe(row, \"address\")\n",
    "    city = _safe(row, \"city\")\n",
    "    stock = _safe(row, \"stock_code\")\n",
    "    job = _safe(row, \"job\")\n",
    "\n",
    "    client = (fn + \" \" + ln).strip() or \"Customer\"\n",
    "    lines = [\n",
    "        \"INVOICE\",\n",
    "        f\"Date: {inv_date}\".strip(),\n",
    "        f\"Customer: {client}\".strip(),\n",
    "        f\"Company: {job}\".strip(),\n",
    "        f\"Address: {addr} {city}\".strip(),\n",
    "        f\"Email: {email}\".strip(),\n",
    "        f\"Item: {product_id} {stock}\".strip(),\n",
    "        f\"Qty: {qty}\".strip(),\n",
    "        f\"Unit price: {amount}\".strip(),\n",
    "    ]\n",
    "    lines = [ln for ln in lines if ln.split(\":\")[-1].strip() != \"\" and ln.strip() != \"Address:\"]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def load_invoices_2(path: Path, max_rows: int | None = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    missing = [c for c in KAGGLE_REQUIRED if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing invoice2 columns: {missing}\")\n",
    "    if max_rows:\n",
    "        df = df.head(max_rows)\n",
    "    texts = df.apply(lambda r: invoice2_row_to_text(r.to_dict()), axis=1)\n",
    "    out = pd.DataFrame({\"text\": texts.map(basic_clean)})\n",
    "    out[\"doc_type\"] = \"INVOICE\"\n",
    "    out[\"source\"] = \"INVOICE_SET_2\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d988b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_arxiv(path: Path, max_rows: int | None = None) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    if path.suffix.lower() == \".jsonl\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if max_rows and i >= max_rows:\n",
    "                    break\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                obj = json.loads(line)\n",
    "                title = str(obj.get(\"title\", \"\")).strip()\n",
    "                abstract = str(obj.get(\"abstract\", \"\")).strip()\n",
    "                txt = (title + \"\\n\\n\" + abstract).strip()\n",
    "                if txt:\n",
    "                    rows.append(txt)\n",
    "    else:\n",
    "        # tolerate JSONL in .json\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            head = f.read(4096)\n",
    "        if \"\\n\" in head and head.lstrip().startswith(\"{\"):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if max_rows and i >= max_rows:\n",
    "                        break\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    obj = json.loads(line)\n",
    "                    title = str(obj.get(\"title\", \"\")).strip()\n",
    "                    abstract = str(obj.get(\"abstract\", \"\")).strip()\n",
    "                    txt = (title + \"\\n\\n\" + abstract).strip()\n",
    "                    if txt:\n",
    "                        rows.append(txt)\n",
    "        else:\n",
    "            data = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "            if isinstance(data, list):\n",
    "                it = data\n",
    "            else:\n",
    "                it = data.get(\"data\", [])\n",
    "            for obj in it:\n",
    "                if max_rows and len(rows) >= max_rows:\n",
    "                    break\n",
    "                title = str(obj.get(\"title\", \"\")).strip()\n",
    "                abstract = str(obj.get(\"abstract\", \"\")).strip()\n",
    "                txt = (title + \"\\n\\n\" + abstract).strip()\n",
    "                if txt:\n",
    "                    rows.append(txt)\n",
    "\n",
    "    out = pd.DataFrame({\"text\": pd.Series(rows).map(basic_clean)})\n",
    "    out[\"doc_type\"] = \"SCIENTIFIC_PAPER\"\n",
    "    out[\"source\"] = \"ARXIV\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0afec61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3113730, 4)\n",
      "doc_type\n",
      "SCIENTIFIC_PAPER    2872037\n",
      "EMAIL                221693\n",
      "INVOICE               20000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Build\n",
    "parts = []\n",
    "\n",
    "if ENRON_CSV.exists():\n",
    "    parts.append(load_enron(ENRON_CSV, max_rows=None))\n",
    "if INVOICE1_CSV.exists():\n",
    "    parts.append(load_invoices_1(INVOICE1_CSV, max_rows=None))\n",
    "if INVOICE2_CSV.exists():\n",
    "    parts.append(load_invoices_2(INVOICE2_CSV, max_rows=None))\n",
    "if ARXIV_PATH.exists():\n",
    "    parts.append(load_arxiv(ARXIV_PATH, max_rows=None))\n",
    "\n",
    "if not parts:\n",
    "    raise FileNotFoundError(\"No datasets found in input/\")\n",
    "\n",
    "df = pd.concat(parts, ignore_index=True)\n",
    "df[\"text\"] = df[\"text\"].astype(str).map(basic_clean)\n",
    "df = df[df[\"text\"].str.len() > 0].copy()\n",
    "\n",
    "df[\"doc_id\"] = df[\"text\"].map(stable_id)\n",
    "df = df.drop_duplicates(subset=[\"doc_id\"]).reset_index(drop=True)\n",
    "df = filter_by_length(df, \"text\", min_chars=80, max_chars=12000)\n",
    "\n",
    "print(df.shape)\n",
    "print(df[\"doc_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474e987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>source</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "      <td>EMAIL</td>\n",
       "      <td>ENRON</td>\n",
       "      <td>2631cf6b64a37507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Randy, Can you send me a schedule of the salar...</td>\n",
       "      <td>EMAIL</td>\n",
       "      <td>ENRON</td>\n",
       "      <td>89ce8dde64d2b23b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Please cc the following distribution list with...</td>\n",
       "      <td>EMAIL</td>\n",
       "      <td>ENRON</td>\n",
       "      <td>6eb7c06288a2d944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1. login: pallen pw: ke9davis I don't think th...</td>\n",
       "      <td>EMAIL</td>\n",
       "      <td>ENRON</td>\n",
       "      <td>fa7758f7c5f01bde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>---------------------- Forwarded by Phillip K ...</td>\n",
       "      <td>EMAIL</td>\n",
       "      <td>ENRON</td>\n",
       "      <td>f243c5ccecb4af42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135380</th>\n",
       "      <td>On the origin of the irreversibility line in t...</td>\n",
       "      <td>SCIENTIFIC_PAPER</td>\n",
       "      <td>ARXIV</td>\n",
       "      <td>99175dfe1f070e81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135381</th>\n",
       "      <td>Nonlinear Response of HTSC Thin Film Microwave...</td>\n",
       "      <td>SCIENTIFIC_PAPER</td>\n",
       "      <td>ARXIV</td>\n",
       "      <td>920c9e984b480968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135382</th>\n",
       "      <td>Critical State Flux Penetration and Linear Mic...</td>\n",
       "      <td>SCIENTIFIC_PAPER</td>\n",
       "      <td>ARXIV</td>\n",
       "      <td>099995b18c91d508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135383</th>\n",
       "      <td>Density of States and NMR Relaxation Rate in A...</td>\n",
       "      <td>SCIENTIFIC_PAPER</td>\n",
       "      <td>ARXIV</td>\n",
       "      <td>c2ce38d3013486fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135384</th>\n",
       "      <td>Ginzburg Landau theory for d-wave pairing and ...</td>\n",
       "      <td>SCIENTIFIC_PAPER</td>\n",
       "      <td>ARXIV</td>\n",
       "      <td>204091aab8f9d9a9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3113730 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text          doc_type  \\\n",
       "1        Traveling to have a business meeting takes the...             EMAIL   \n",
       "3        Randy, Can you send me a schedule of the salar...             EMAIL   \n",
       "6        Please cc the following distribution list with...             EMAIL   \n",
       "8        1. login: pallen pw: ke9davis I don't think th...             EMAIL   \n",
       "9        ---------------------- Forwarded by Phillip K ...             EMAIL   \n",
       "...                                                    ...               ...   \n",
       "3135380  On the origin of the irreversibility line in t...  SCIENTIFIC_PAPER   \n",
       "3135381  Nonlinear Response of HTSC Thin Film Microwave...  SCIENTIFIC_PAPER   \n",
       "3135382  Critical State Flux Penetration and Linear Mic...  SCIENTIFIC_PAPER   \n",
       "3135383  Density of States and NMR Relaxation Rate in A...  SCIENTIFIC_PAPER   \n",
       "3135384  Ginzburg Landau theory for d-wave pairing and ...  SCIENTIFIC_PAPER   \n",
       "\n",
       "        source            doc_id  \n",
       "1        ENRON  2631cf6b64a37507  \n",
       "3        ENRON  89ce8dde64d2b23b  \n",
       "6        ENRON  6eb7c06288a2d944  \n",
       "8        ENRON  fa7758f7c5f01bde  \n",
       "9        ENRON  f243c5ccecb4af42  \n",
       "...        ...               ...  \n",
       "3135380  ARXIV  99175dfe1f070e81  \n",
       "3135381  ARXIV  920c9e984b480968  \n",
       "3135382  ARXIV  099995b18c91d508  \n",
       "3135383  ARXIV  c2ce38d3013486fe  \n",
       "3135384  ARXIV  204091aab8f9d9a9  \n",
       "\n",
       "[3113730 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "предательство, измена, инстаграм измена, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5c2e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_target: 20000\n",
      "inv_target: 20000\n",
      "cap used: 20000\n",
      "Balanced counts: {'EMAIL': 20000, 'INVOICE': 20000, 'SCIENTIFIC_PAPER': 20000}\n"
     ]
    }
   ],
   "source": [
    "invoice1_df = pd.read_csv(INVOICE1_CSV)\n",
    "invoice2_df = pd.read_csv(INVOICE2_CSV)\n",
    "\n",
    "inv_target = len(invoice1_df) + len(invoice2_df)\n",
    "print(\"inv_target:\", inv_target)\n",
    "\n",
    "if inv_target <= 0:\n",
    "    raise ValueError(\"Invoice target is 0. Check invoice datasets loading.\")\n",
    "\n",
    "classes = [\"EMAIL\", \"INVOICE\", \"SCIENTIFIC_PAPER\"]\n",
    "\n",
    "# sanity\n",
    "for cls in classes:\n",
    "    n = int((df[\"doc_type\"] == cls).sum())\n",
    "    if n == 0:\n",
    "        raise ValueError(f\"No rows for class {cls}\")\n",
    "    if cls != \"INVOICE\" and n < inv_target:\n",
    "        print(f\"Warning: {cls} has only {n} (< inv_target {inv_target}). Will cap to {n} instead.\")\n",
    "\n",
    "cap = min(\n",
    "    inv_target,\n",
    "    int((df[\"doc_type\"] == \"EMAIL\").sum()),\n",
    "    int((df[\"doc_type\"] == \"SCIENTIFIC_PAPER\").sum()),\n",
    ")\n",
    "\n",
    "# keep invoices as-is, cap the other two\n",
    "email_cap = df[df[\"doc_type\"] == \"EMAIL\"].sample(n=cap, random_state=42)\n",
    "paper_cap = df[df[\"doc_type\"] == \"SCIENTIFIC_PAPER\"].sample(n=cap, random_state=42)\n",
    "\n",
    "inv_df = df[df[\"doc_type\"] == \"INVOICE\"].copy()\n",
    "\n",
    "# if invoices are larger than cap, cap them too (for full balance)\n",
    "inv_cap = inv_df.sample(n=cap, random_state=42) if len(inv_df) > cap else inv_df\n",
    "\n",
    "df_bal = pd.concat([email_cap, inv_cap, paper_cap], ignore_index=True)\n",
    "df_bal = df_bal.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"inv_target:\", inv_target)\n",
    "print(\"cap used:\", cap)\n",
    "print(\"Balanced counts:\", df_bal[\"doc_type\"].value_counts().to_dict())\n",
    "\n",
    "df = df_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "946aa02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\viach\\Downloads\\document-classifier-portfolio-v2\\data\\processed\\train.csv C:\\Users\\viach\\Downloads\\document-classifier-portfolio-v2\\data\\processed\\val.csv C:\\Users\\viach\\Downloads\\document-classifier-portfolio-v2\\data\\processed\\test.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, tmp_df = train_test_split(df, test_size=0.30, random_state=42, stratify=df[\"doc_type\"])\n",
    "val_df, test_df = train_test_split(tmp_df, test_size=0.50, random_state=42, stratify=tmp_df[\"doc_type\"])\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "train_path = PROCESSED_DIR / \"train.csv\"\n",
    "val_path = PROCESSED_DIR / \"val.csv\"\n",
    "test_path = PROCESSED_DIR / \"test.csv\"\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"Saved:\", train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ae872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
